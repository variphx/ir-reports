\subsection{Phân tích tài liệu}
Phân tích tài liệu là bước khởi đầu có tính nền tảng trong quá trình truy xuất thông tin - một giai đoạn được xem như cánh cửa đầu tiên dẫn ngôn ngữ tự nhiên đến thế giới hình thức và toán học. Về bản chất, đây là quá trình \textbf{tiền xử lý văn bản} (text pre-processing), nhằm chuyển hóa dữ liệu ngôn ngữ thô - vốn thường không có cấu trúc và mang nhiều yếu tố nhiễu - thành dạng thức chuẩn hóa, dễ dàng cho việc mô hình hóa và tính toán về sau.

Trong thế giới của dữ liệu văn bản, tồn tại rất nhiều từ ngữ có tần suất xuất hiện cao nhưng lại \textbf{ít mang giá trị ngữ nghĩa phân biệt}, điển hình như các \textbf{stopwords} (ví dụ: \textit{a}, \textit{the}, \textit{in}, \textit{on}, \dots), các biểu thức cú pháp lặp lại, hay các ký hiệu đặc biệt gây nhiễu. Nếu không được xử lý triệt để, những thành phần này sẽ làm tăng kích thước không cần thiết của mô hình, gây suy giảm hiệu suất và độ chính xác trong quá trình truy vấn.

Bên cạnh đó, văn bản ngôn ngữ tự nhiên còn thường xuyên chứa các biến thể về mặt hình thức, dù mang cùng một ý nghĩa. Chẳng hạn, nếu trong tập từ vựng \(V\) của hệ thống chỉ tồn tại từ ``USA'', nhưng người dùng lại nhập truy vấn dưới dạng ``U.S.A'', thì nếu không có bước chuẩn hóa - như loại bỏ dấu chấm và ghép từ - thì hệ thống sẽ không thể nhận ra hai biểu thức này là tương đương về ngữ nghĩa. Hệ quả là tài liệu có nội dung liên quan sẽ không được truy xuất, dẫn đến việc đánh mất thông tin giá trị.

Vì vậy, \textbf{phân tích và xử lý văn bản không chỉ là một bước kỹ thuật, mà còn là một quá trình ``làm sạch'' và ``gột rửa'' ngôn ngữ}, đưa nó về dạng tinh giản và thuần túy hơn, để từ đó, những biểu diễn toán học phía sau có thể hoạt động hiệu quả, chính xác và phản ánh đúng nhu cầu thông tin của người dùng. Đây chính là bước đi đầu tiên nhưng mang ý nghĩa quyết định trong việc xây dựng một hệ thống truy xuất thông tin thông minh, mạnh mẽ và giàu tính ngữ nghĩa.

\subsubsection{Tokenization}
\textbf{Tokenization}, hay còn gọi là quá trình tách từ, là một bước cơ bản nhưng thiết yếu trong chuỗi tiền xử lý văn bản, đóng vai trò như cánh cổng đầu tiên đưa ngôn ngữ tự nhiên bước vào thế giới hình thức của truy xuất thông tin. Về nguyên lý, đây là hành động phân chia một văn bản thành những đơn vị nhỏ hơn - gọi là \textbf{token} - thường được xác định dựa trên các dấu phân cách như khoảng trắng, dấu câu hoặc ký tự đặc biệt. Trong tiếng Anh, việc phân tách này chủ yếu dựa theo dấu cách giữa các từ.

Một \textbf{token} được hiểu là biểu hiện cụ thể của một chuỗi ký tự - ví dụ như `the', `boundary', hay `plate' - và nó được xem là một đơn vị có ý nghĩa trong quá trình xử lý ngôn ngữ. Khi nhiều token có cùng chuỗi ký tự, chúng được gom vào cùng một lớp trừu tượng gọi là \textbf{type}. Trên cơ sở đó, khi một type được hệ thống chấp nhận và lưu trữ trong \textbf{dictionary}, nó trở thành một \textbf{term} - tức là một từ khóa chính thức sẽ được dùng trong lập chỉ mục và truy vấn.

Lấy ví dụ với câu văn từ tài liệu số 3 trong tập Cranfield:
\textit{the boundary layer in simple shear flow past a flat plate . the boundary layer equations are presented for steady incompressible flow with no pressure gradient .}
Khi thực hiện tách từ, ta thu được 27 token, bao gồm hai lần xuất hiện của cụm 'the boundary layer' và từ 'flow'. Tuy nhiên, về mặt type thì chỉ có khoảng 22 loại khác nhau, và số lượng term được thêm vào từ điển còn phụ thuộc vào các bước xử lý tiếp theo như loại bỏ stopword hay chuẩn hóa từ vựng.

\subsubsection{Lowercase}
Một bước quan trọng trong tiến trình tiền xử lý văn bản là chuyển đổi tất cả ký tự về chữ thường (lowercasing), nhằm loại bỏ sự phân biệt giữa chữ hoa và chữ thường - vốn là đặc trưng không cần thiết trong hầu hết các hệ thống truy xuất thông tin. Việc này giúp đảm bảo rằng những từ có hình thức khác nhau nhưng mang cùng một ý nghĩa sẽ được xử lý thống nhất. Chẳng hạn, hai biểu thức 'Vietnam' và ''vietnam nếu không được chuẩn hóa sẽ bị xem là hai term hoàn toàn khác biệt, dù về mặt ngữ nghĩa, chúng là một.

Trong một ví dụ minh họa đơn giản, ta có một tài liệu từ tập Cranfield sau khi thực hiện bước tách từ (tokenization):
\textit{The, boundary, layer, in, simple, shear, flow, past, a, flat, plate, ., the, boundary, layer, equations, are, presented, for, steady, incompressible, flow, with, no, pressure, gradient, .}

Sau khi áp dụng phép chuyển đổi về chữ thường, văn bản được chuẩn hóa thành:
\textit{the, boundary, layer, in, simple, shear, flow, past, a, flat, plate, ., the, boundary, layer, equations, are, presented, for, steady, incompressible, flow, with, no, pressure, gradient, .}

Qua đó có thể thấy, thao tác hạ chuẩn chữ viết không làm thay đổi nội dung thông tin, nhưng lại mang ý nghĩa lớn về mặt hình thức, giúp mô hình truy xuất tránh được những sai lệch không cần thiết trong quá trình so khớp và tính toán. Đây là một bước nhỏ về mặt kỹ thuật, nhưng lại là một bước đi dài trong hành trình đưa ngôn ngữ về trạng thái chuẩn mực và tối ưu cho truy xuất.

\subsubsection{Loại bỏ punctuations}
\textbf{Punctuation removal} - tức việc loại bỏ các ký tự dấu câu - là một thao tác tiền xử lý có vai trò tinh lọc văn bản, giúp loại bỏ những thành phần ký hiệu không mang lại giá trị đáng kể trong quá trình truy xuất thông tin. Tập hợp các dấu câu thường gặp bao gồm: \texttt{! , . ? : ; - — ( ) [ ] { } < > ' " … / \textbackslash @ \# \$ \% \^{} \& * + = \_ ` \textasciitilde}, vốn được sử dụng để định hình cấu trúc câu trong văn bản tự nhiên, nhưng trong bối cảnh truy vấn, chúng lại trở thành rào cản đối với sự chuẩn hóa và đối sánh ngữ nghĩa.

Thật vậy, dấu câu không chỉ hiếm khi đóng góp ý nghĩa nội dung rõ ràng, mà còn có thể gây ra sự phân mảnh không mong muốn giữa các từ. Một ví dụ tiêu biểu là sự khác biệt giữa hai biểu thức flow'' và flow.'' - nếu dấu chấm không được loại bỏ, hệ thống sẽ xem đây là hai term hoàn toàn riêng biệt, từ đó làm giảm độ chính xác và khả năng truy xuất hiệu quả.

Trong minh họa sau, một văn bản từ tài liệu Cranfield số 3 với các token như:
\begin{center}
    \textit{the, boundary, layer, ...,plate, ., the, ..., pressure, gradient, .}
\end{center}

Sau khi áp dụng thao tác loại bỏ dấu câu, sẽ được chuẩn hóa thành:
\begin{center}
    \textit{the, boundary, layer, ...,plate, the, ..., pressure, gradient}
\end{center}

Từ đó, có thể thấy rằng việc loại bỏ dấu câu không chỉ làm đơn giản hóa biểu diễn văn bản mà còn mang lại sự chính xác cần thiết trong việc khớp truy vấn và tài liệu. Đây là một bước đi tinh tế nhưng không thể thiếu trong quá trình biến ngôn ngữ tự nhiên thành dữ liệu có cấu trúc, giúp các mô hình truy xuất vận hành mượt mà và hiệu quả hơn.

\subsubsection{Loại bỏ stopwords}
\textbf{Stopword removal}, hay còn gọi là việc loại bỏ các từ dừng, là một thao tác quan trọng trong chuỗi tiền xử lý văn bản, nhằm tinh gọn dữ liệu ngôn ngữ và loại trừ những thành phần không mang giá trị phân biệt cao trong quá trình truy xuất thông tin. Stopwords là những từ xuất hiện với tần suất dày đặc trong hầu hết các tài liệu - như \textit{a, an, is, in, of, the}, \dots - nhưng lại hiếm khi góp phần đáng kể vào việc xác định chủ đề hay nội dung cốt lõi của văn bản.

Việc giữ lại các từ như vậy không chỉ làm tăng kích thước không cần thiết của các vectơ biểu diễn, mà còn gây tốn kém về mặt tính toán do làm phình to không gian đặc trưng một cách vô nghĩa. Trong các mô hình biểu diễn toán học như ma trận term-document hoặc không gian vector, sự hiện diện của các stopword làm giảm độ sắc nét trong việc phân tách các khái niệm, và đôi khi còn gây nhiễu trong quá trình truy vấn. Do đó, loại bỏ stopword không đơn thuần là một hành vi cắt giảm, mà là một hành động thanh lọc - giữ lại những gì mang tính bản chất và loại trừ những yếu tố rườm rà.

Tuy nhiên, việc xác định stopword không phải lúc nào cũng mang tính tuyệt đối. Tùy vào ngữ cảnh sử dụng, lĩnh vực chuyên môn hay mục tiêu phân tích, tập hợp các từ cần loại bỏ có thể thay đổi linh hoạt. Một từ có thể vô nghĩa trong tài liệu này, nhưng lại mang tính chất quan trọng trong tài liệu khác.

Trong ví dụ từ tài liệu số 3, sau khi thực hiện thao tác loại bỏ stopwords, ta thu được kết quả sau:
\textit{boundary, layer, simple, shear, flow, past, flat, plate, boundary, layer, equations, presented, steady, incompressible, flow, pressure, gradient}.

Kết quả này phản ánh rõ ràng nội dung chính của đoạn văn, trong khi những từ còn lại - dù đóng vai trò ngữ pháp - không còn cần thiết cho mục tiêu truy xuất. Vậy nên, stopword removal chính là nghệ thuật của sự tinh lọc - nơi ngôn ngữ được bóc tách, gạn lọc để lại những hạt nhân ngữ nghĩa giàu giá trị cho quá trình phân tích và truy vấn.

\subsubsection{Stemming}
\textbf{Stemming}, hay còn gọi là quá trình ``lấy gốc từ'', là một thao tác tiền xử lý mang tính nền tảng trong truy xuất thông tin, với mục tiêu chuẩn hóa các từ ngữ về dạng cơ bản nhất của chúng. Về bản chất, quá trình này tìm cách loại bỏ những biến thể ngữ pháp không cần thiết như hậu tố hoặc tiền tố, nhằm thu về một dạng từ thống nhất - thường được gọi là \textbf{stem}. Điều này không chỉ giúp giảm thiểu kích thước từ vựng mà còn tăng cường khả năng tổng quát hóa trong quá trình truy vấn, cho phép người dùng không cần nắm rõ mọi biến thể ngôn ngữ vẫn có thể tiếp cận đúng tài liệu họ cần.

Thật vậy, trong kho dữ liệu văn bản tự nhiên, sự phong phú về hình thái từ có thể tạo ra hàng loạt phiên bản khác nhau của cùng một ý nghĩa. Các động từ như \textit{presented, presenting, presents} dù mang hình thức khác nhau về ngữ pháp, nhưng tựu trung lại đều biểu đạt hành động ``trình bày''. Với kỹ thuật stemming, những biến thể này sẽ được đưa về dạng gốc duy nhất là \textit{present} - thông qua thao tác cắt bỏ các hậu tố như \textit{-ed}, \textit{-ing}, \textit{-s}. Các công cụ thực hiện quá trình này được gọi là \textbf{stemmer}, và dù phương pháp này mang tính quy tắc cứng nhắc, nó mang lại hiệu suất xử lý nhanh và phù hợp với những hệ thống yêu cầu tốc độ cao hơn độ chính xác hình thái.

Tuy nhiên, nếu nhu cầu về độ chính xác được đặt lên hàng đầu, người ta thường sử dụng một kỹ thuật tinh vi hơn mang tên \textbf{lemmatization}. Khác với stemming vốn chỉ dựa vào quy tắc ký tự đơn giản, lemmatization sử dụng từ điển hoặc tập ontology để xác định dạng gốc thực sự của một từ - được gọi là \textbf{lemma}. Nhờ đó, không chỉ các biến thể như \textit{goes}, \textit{going}, \textit{went} được đưa về \textit{go}, mà ngay cả các cặp danh từ bất quy tắc như \textit{mouse} và \textit{mice} cũng có thể được đồng nhất hóa. Quá trình này yêu cầu tra cứu từ vựng phức tạp hơn, đồng nghĩa với chi phí thời gian lớn hơn, nhưng đổi lại là độ chính xác vượt trội trong biểu diễn ngôn ngữ.

Tựu trung lại, việc chuẩn hóa hình thái từ ngữ không chỉ mang lại lợi ích rõ rệt trong việc giảm tải số lượng từ khóa cần lập chỉ mục, mà còn tăng cường tính nhất quán trong toàn bộ quá trình truy xuất. Khi số lượng từ được rút gọn và quy về dạng gốc, quá trình xây dựng chỉ mục trở nên nhẹ nhàng hơn, đồng thời tốc độ truy vấn cũng được cải thiện đáng kể.

Vẫn xét ví dụ như trên, kết quả xử lý lúc này sẽ là:
\textit{boundari, layer, simpl, shear, flow, past, flat, plate, boundari, layer, equat, present, steadi, incompress, flow, pressur, gradient}.

Mỗi từ còn lại đều mang trọng lượng ngữ nghĩa rõ rệt, phản ánh trực tiếp nội dung của văn bản mà không bị phân tán bởi các yếu tố ngữ pháp không cần thiết. Đây chính là minh chứng cho vai trò thiết yếu của stemming trong việc tinh lọc ngôn ngữ và tối ưu hóa hiệu suất của hệ thống truy xuất thông tin hiện đại.